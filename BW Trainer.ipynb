{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x124558d30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "\n",
    "torch.manual_seed(69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    filename=\"app.log\",         # File to log to\n",
    "    level=logging.INFO,         # Logging level\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",  # Log message format\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\" # Date and time format\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downscaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Downscaling all images in the dataset. \n",
    "\n",
    "def downscale_images(input_root, output_root, downscale_factor):\n",
    "    for root, dirs, files in os.walk(input_root):\n",
    "        # Create corresponding directory in the output structure\n",
    "        relative_path = os.path.relpath(root, input_root)\n",
    "        output_dir = os.path.join(output_root, relative_path)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        for file in tqdm(files):\n",
    "            if file.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'tiff', 'gif')):\n",
    "                input_path = os.path.join(root, file)\n",
    "                output_path = os.path.join(output_dir, file)\n",
    "                \n",
    "                try:\n",
    "                    # Read and downscale the image\n",
    "                    image = cv2.imread(input_path)\n",
    "                    if image is not None:\n",
    "                        downsampled_image = cv2.resize(\n",
    "                            image,\n",
    "                            None,\n",
    "                            fx=1/downscale_factor,\n",
    "                            fy=1/downscale_factor,\n",
    "                            interpolation=cv2.INTER_AREA\n",
    "                        )\n",
    "                        # Save the downscaled image\n",
    "                        cv2.imwrite(output_path, downsampled_image)\n",
    "                        #print(f\"Processed: {input_path} -> {output_path}\")\n",
    "                    else:\n",
    "                        #print(f\"Failed to read: {input_path}\")\n",
    "                        pass\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {input_path}: {e}\")\n",
    "            else:\n",
    "                #print(f\"Skipped non-image file: {file}\")\n",
    "                pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_root = \"ProperDataset\"  # Input dataset root directory\n",
    "    output_root = \"training_images\"  # Output dataset root directory\n",
    "    downscale_factor = 10  # Adjust this factor as needed\n",
    "    downscale_images(input_root, output_root, downscale_factor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "splitfolders.ratio(\"training_images\", output=\"Dataset\",\n",
    "    seed=1337, ratio=(.8, .1, .1), group_prefix=None, move=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to dataset\n",
    "data_dir = \"Dataset\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "val_dir = os.path.join(data_dir, \"val\")\n",
    "test_dir = os.path.join(data_dir, \"test\") \n",
    "\n",
    "# Define data transformations\n",
    "transform = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)), \n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor() \n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    \"test\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Load datasets\n",
    "dataset_dict = {\n",
    "    \"train\": datasets.ImageFolder(train_dir, transform=transform[\"train\"]),\n",
    "    \"val\": datasets.ImageFolder(val_dir, transform=transform[\"val\"]),\n",
    "    \"test\": datasets.ImageFolder(test_dir, transform=transform[\"test\"]) \n",
    "}\n",
    "\n",
    "# Create DataLoaders\n",
    "dataloaders = {\n",
    "    \"train\": DataLoader(dataset_dict[\"train\"], batch_size=32, shuffle=True, num_workers=4),\n",
    "    \"val\": DataLoader(dataset_dict[\"val\"], batch_size=32, shuffle=False, num_workers=4),\n",
    "    \"test\": DataLoader(dataset_dict[\"test\"], batch_size=32, shuffle=False, num_workers=4)  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Model from models.ipynb and paste it here\n",
    "## The trainer is 224x224, BW It uses STRIDE 3.3's Architecture. \n",
    "\n",
    "\n",
    "class FastCNN(nn.Module):\n",
    "    def __init__(self, num_categories, img_height=224, img_width=224):\n",
    "        super(FastCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=32,\n",
    "            kernel_size=3,\n",
    "            padding=0\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32,\n",
    "            out_channels=32,\n",
    "            kernel_size=3,\n",
    "            padding=0\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=32,\n",
    "            out_channels=32,\n",
    "            kernel_size=3,\n",
    "            padding=0\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=32,\n",
    "            out_channels=8,\n",
    "            kernel_size=3,\n",
    "            padding=0\n",
    "        )\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(1152, 128)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc2 = nn.Linear(128, num_categories)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # x : 224 * 224 * 1\n",
    "\n",
    "        x = torch.tanh(self.conv1(x))  # 222 * 222 * 32\n",
    "        x = F.avg_pool2d(x, 2) # 111 * 111 * 32\n",
    "        \n",
    "        x = torch.tanh(self.conv2(x)) # 109 * 109 * 32\n",
    "        x = F.avg_pool2d(x, 2) # 54 * 54 * 32\n",
    "        \n",
    "        x = torch.tanh(self.conv3(x)) # 52 * 52 * 32\n",
    "        x = F.avg_pool2d(x, 2) # 26 * 26 * 32\n",
    "\n",
    "        x = torch.tanh(self.conv4(x)) # 24 * 24 * 8\n",
    "        x = F.avg_pool2d(x, 2) # 12 * 12 * 8\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Will flatten to (32, 1152)\n",
    "        \n",
    "        # Dense layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def get_model(num_categories, img_height=224, img_width=224):\n",
    "    return FastCNN(num_categories, img_height, img_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "fast = FastCNN(5, 148, 138)\n",
    "model = get_model(5, 148, 138)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use it to load the trained weights or train it yourself in the below cell\n",
    "# model.load_state_dict( torch.load(\"model_weights.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [15:09<00:00, 90.93s/it]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "g = \"v4.1\"\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "noop_class_index = dataset_dict[\"train\"].class_to_idx[\"noop\"]  # Index of the \"noop\" class\n",
    "\n",
    "# Get indices for all \"noop\" samples\n",
    "noop_indices = [i for i, (_, label) in enumerate(dataset_dict[\"train\"].samples) if label == noop_class_index]\n",
    "\n",
    "def get_random_noop_subset(base_dataset, noop_indices, num_samples=1000):\n",
    "    # Randomly sample 1000 indices from the \"noop\" indices\n",
    "    selected_noop_indices = random.sample(noop_indices, num_samples)\n",
    "\n",
    "    # Combine the noop subset with the rest of the dataset\n",
    "    non_noop_indices = [i for i in range(len(base_dataset)) if i not in noop_indices]\n",
    "    final_indices = non_noop_indices + selected_noop_indices\n",
    "\n",
    "    # Return a Subset dataset\n",
    "    return Subset(base_dataset, final_indices)\n",
    "\n",
    "for epoch in trange(1, num_epochs+1):\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    current_train_dataset = get_random_noop_subset(dataset_dict[\"train\"], noop_indices)\n",
    "\n",
    "    # Create a DataLoader\n",
    "    train_loader = DataLoader(current_train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += torch.sum(preds == labels.data)\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(dataloaders[\"train\"].dataset)\n",
    "    train_acc = correct.double() / total\n",
    "    logging.info(f\"{epoch} : Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        torch.save(model.state_dict(), f\"Trained_Weights/{g} {epoch}th.pth\")\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloaders[\"val\"]:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += torch.sum(preds == labels.data)\n",
    "                total += labels.size(0)\n",
    "\n",
    "        val_loss = running_loss / len(dataloaders[\"val\"].dataset)\n",
    "        val_acc = correct.double() / total\n",
    "        logging.info(f\"{epoch} :Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in dataloaders[\"test\"]:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += torch.sum(preds == labels.data)\n",
    "        total += labels.size(0)\n",
    "\n",
    "test_loss = running_loss / len(dataloaders[\"test\"].dataset)\n",
    "test_acc = correct.double() / total\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mss\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from pynput.keyboard import Controller, Key\n",
    "from PIL import Image\n",
    "import time\n",
    "# Initialize keyboard controller\n",
    "keyboard = Controller()\n",
    "\n",
    "# Define transformations for the captured image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Monitor region for screen capture\n",
    "monitor = {\n",
    "    \"left\": 350,  # x-coordinate for the top-left corner\n",
    "    \"top\": 180,   # y-coordinate for the top-left corner\n",
    "    \"width\": 740, # width of the capture region\n",
    "    \"height\": 690 # height of the capture region\n",
    "}\n",
    "acc=set()\n",
    "# Key mapping for predictions\n",
    "key_map = {\n",
    "    \"left\": Key.left,\n",
    "    \"right\": Key.right,\n",
    "    \"up\": Key.up,\n",
    "    \"down\": Key.down\n",
    "}\n",
    "\n",
    "# Function to simulate key presses\n",
    "def press_key(action):\n",
    "    if action in key_map:\n",
    "        keyboard.press(key_map[action])\n",
    "        keyboard.release(key_map[action])\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Capture and process the screen\n",
    "with mss.mss() as sct:\n",
    "    i = 0\n",
    "    while True:\n",
    "        # Capture screen region\n",
    "        screenshot = sct.grab(monitor)\n",
    "        i+=1\n",
    "        screenshot_np = np.array(screenshot)\n",
    "\n",
    "        # Resize and convert the image\n",
    "        downscale_factor = 2\n",
    "        resized_image = cv2.resize(\n",
    "            screenshot_np,\n",
    "            None,\n",
    "            fx=1 / downscale_factor,\n",
    "            fy=1 / downscale_factor,\n",
    "            interpolation=cv2.INTER_AREA\n",
    "        )\n",
    "        rgb_image = cv2.cvtColor(resized_image, cv2.COLOR_BGRA2RGB)\n",
    "\n",
    "        # Preprocess image\n",
    "        img_tensor = transform(Image.fromarray(rgb_image)).unsqueeze(0) # Add batch dimension\n",
    "\n",
    "        # Make predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(img_tensor)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            prediction_idx = predicted.item()\n",
    "\n",
    "        # Map prediction index to action\n",
    "        class_names = dataset_dict['train'].classes\n",
    "        \n",
    "        action = class_names[prediction_idx]\n",
    "\n",
    "        # Simulate key press if necessary\n",
    "        acc.add(action)\n",
    "        if action != \"noop\":\n",
    "            acc.add(action)\n",
    "            press_key(action)\n",
    "            time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
