{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "\n",
    "torch.manual_seed(69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    filename=\"app.log\",         # File to log to\n",
    "    level=logging.INFO,         # Logging level\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",  # Log message format\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\" # Date and time format\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downscaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Downscaling all images in the dataset. \n",
    "\n",
    "def downscale_images(input_root, output_root, downscale_factor):\n",
    "    for root, dirs, files in os.walk(input_root):\n",
    "        # Create corresponding directory in the output structure\n",
    "        relative_path = os.path.relpath(root, input_root)\n",
    "        output_dir = os.path.join(output_root, relative_path)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        for file in tqdm(files):\n",
    "            if file.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'tiff', 'gif')):\n",
    "                input_path = os.path.join(root, file)\n",
    "                output_path = os.path.join(output_dir, file)\n",
    "                try:\n",
    "                    # Read and downscale the image\n",
    "                    image = cv2.imread(input_path)\n",
    "                    if image is not None:\n",
    "                        downsampled_image = cv2.resize(\n",
    "                            image,\n",
    "                            None,\n",
    "                            fx=1/downscale_factor,\n",
    "                            fy=1/downscale_factor,\n",
    "                            interpolation=cv2.INTER_AREA\n",
    "                        )\n",
    "                        # Save the downscaled image\n",
    "                        cv2.imwrite(output_path, downsampled_image)\n",
    "                        #print(f\"Processed: {input_path} -> {output_path}\")\n",
    "                    else:\n",
    "                        #print(f\"Failed to read: {input_path}\")\n",
    "                        pass\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {input_path}: {e}\")\n",
    "            else:\n",
    "                #print(f\"Skipped non-image file: {file}\")\n",
    "                pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_root = \"ProperDataset\"  # Input dataset root directory\n",
    "    output_root = \"training_images\"  # Output dataset root directory\n",
    "    downscale_factor = 10  # Adjust this factor as needed\n",
    "    downscale_images(input_root, output_root, downscale_factor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "splitfolders.ratio(\"training_images\", output=\"Dataset\",\n",
    "    seed=1337, ratio=(.8, .1, .1), group_prefix=None, move=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to dataset\n",
    "data_dir = \"Dataset\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "val_dir = os.path.join(data_dir, \"val\")\n",
    "test_dir = os.path.join(data_dir, \"test\") \n",
    "\n",
    "# PASTE YOUR TRANSFORMATIONS BELOW\n",
    "transform = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.Resize((64,64)), \n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor() \n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    \"test\": transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Load datasets\n",
    "dataset_dict = {\n",
    "    \"train\": datasets.ImageFolder(train_dir, transform=transform[\"train\"]),\n",
    "    \"val\": datasets.ImageFolder(val_dir, transform=transform[\"val\"]),\n",
    "    \"test\": datasets.ImageFolder(test_dir, transform=transform[\"test\"]) \n",
    "}\n",
    "\n",
    "# Create DataLoaders\n",
    "dataloaders = {\n",
    "    \"train\": DataLoader(dataset_dict[\"train\"], batch_size=32, shuffle=True, num_workers=4),\n",
    "    \"val\": DataLoader(dataset_dict[\"val\"], batch_size=32, shuffle=False, num_workers=4),\n",
    "    \"test\": DataLoader(dataset_dict[\"test\"], batch_size=32, shuffle=False, num_workers=4)  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "fast = FastCNN(5)\n",
    "model = get_model(5)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use it to load the trained weights or train it yourself in the below cell\n",
    "# model.load_state_dict( torch.load(\"model_weights.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [05:38<00:00, 33.82s/it]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "g = \"v4.3\"\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "noop_class_index = dataset_dict[\"train\"].class_to_idx[\"noop\"]  # Index of the \"noop\" class\n",
    "\n",
    "# Get indices for all \"noop\" samples\n",
    "noop_indices = [i for i, (_, label) in enumerate(dataset_dict[\"train\"].samples) if label == noop_class_index]\n",
    "\n",
    "def get_random_noop_subset(base_dataset, noop_indices, num_samples=1000):\n",
    "    # Randomly sample 1000 indices from the \"noop\" indices\n",
    "    selected_noop_indices = random.sample(noop_indices, num_samples)\n",
    "\n",
    "    # Combine the noop subset with the rest of the dataset\n",
    "    non_noop_indices = [i for i in range(len(base_dataset)) if i not in noop_indices]\n",
    "    final_indices = non_noop_indices + selected_noop_indices\n",
    "\n",
    "    # Return a Subset dataset\n",
    "    return Subset(base_dataset, final_indices)\n",
    "\n",
    "for epoch in trange(11, num_epochs+1):\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    current_train_dataset = get_random_noop_subset(dataset_dict[\"train\"], noop_indices)\n",
    "\n",
    "    # Create a DataLoader\n",
    "    train_loader = DataLoader(current_train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += torch.sum(preds == labels.data)\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(dataloaders[\"train\"].dataset)\n",
    "    train_acc = correct.double() / total\n",
    "    logging.info(f\"{epoch} : Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        # Save model weights\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        save_path = f\"Trained_Weights/{g}_{epoch}th.pth\"\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "\n",
    "        # Initialize validation metrics\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculations for validation\n",
    "            for inputs, labels in dataloaders[\"val\"]:\n",
    "                # Move inputs and labels to the appropriate device\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Update loss and accuracy\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)  # Get predicted class indices\n",
    "                correct += torch.sum(preds == labels)\n",
    "                total += labels.size(0)\n",
    "\n",
    "        # Calculate validation loss and accuracy\n",
    "        val_loss = running_loss / len(dataloaders[\"val\"].dataset)\n",
    "        val_acc = correct.double() / total\n",
    "\n",
    "        # Log validation metrics\n",
    "        logging.info(f\"Epoch {epoch}: Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in dataloaders[\"test\"]:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += torch.sum(preds == labels.data)\n",
    "        total += labels.size(0)\n",
    "\n",
    "test_loss = running_loss / len(dataloaders[\"test\"].dataset)\n",
    "test_acc = correct.double() / total\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# Capture screen region\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     screenshot \u001b[38;5;241m=\u001b[39m \u001b[43msct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     51\u001b[0m     screenshot_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(screenshot)\n",
      "File \u001b[0;32m~/Desktop/ML Projects/SubSurferAI/venv/lib/python3.12/site-packages/mss/base.py:101\u001b[0m, in \u001b[0;36mMSSBase.grab\u001b[0;34m(self, monitor)\u001b[0m\n\u001b[1;32m     93\u001b[0m     monitor \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m: monitor[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop\u001b[39m\u001b[38;5;124m\"\u001b[39m: monitor[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m: monitor[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m monitor[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m: monitor[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m monitor[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     98\u001b[0m     }\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m lock:\n\u001b[0;32m--> 101\u001b[0m     screenshot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grab_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_cursor \u001b[38;5;129;01mand\u001b[39;00m (cursor \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cursor_impl()):\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge(screenshot, cursor)\n",
      "File \u001b[0;32m~/Desktop/ML Projects/SubSurferAI/venv/lib/python3.12/site-packages/mss/darwin.py:199\u001b[0m, in \u001b[0;36mMSS._grab_impl\u001b[0;34m(self, monitor)\u001b[0m\n\u001b[1;32m    197\u001b[0m             start \u001b[38;5;241m=\u001b[39m row \u001b[38;5;241m*\u001b[39m bytes_per_row\n\u001b[1;32m    198\u001b[0m             end \u001b[38;5;241m=\u001b[39m start \u001b[38;5;241m+\u001b[39m width \u001b[38;5;241m*\u001b[39m bytes_per_pixel\n\u001b[0;32m--> 199\u001b[0m             \u001b[43mcropped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m         data \u001b[38;5;241m=\u001b[39m cropped\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mss\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from pynput.keyboard import Controller, Key\n",
    "from PIL import Image\n",
    "import time\n",
    "# Initialize keyboard controller\n",
    "keyboard = Controller()\n",
    "\n",
    "# PASTE YOUR TRANSFORMATIONS HERE\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)), \n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor() \n",
    "])\n",
    "\n",
    "# Monitor region for screen capture\n",
    "monitor = {\n",
    "    \"left\": 350,  # x-coordinate for the top-left corner\n",
    "    \"top\": 180,   # y-coordinate for the top-left corner\n",
    "    \"width\": 740, # width of the capture region\n",
    "    \"height\": 690 # height of the capture region\n",
    "}\n",
    "acc=set()\n",
    "# Key mapping for predictions\n",
    "key_map = {\n",
    "    \"left\": Key.left,\n",
    "    \"right\": Key.right,\n",
    "    \"up\": Key.up,\n",
    "    \"down\": Key.down\n",
    "}\n",
    "\n",
    "# Function to simulate key presses\n",
    "def press_key(action):\n",
    "    if action in key_map:\n",
    "        keyboard.press(key_map[action])\n",
    "        keyboard.release(key_map[action])\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Capture and process the screen\n",
    "with mss.mss() as sct:\n",
    "    i = 0\n",
    "    while True:\n",
    "        # Capture screen region\n",
    "        screenshot = sct.grab(monitor)\n",
    "        i+=1\n",
    "        screenshot_np = np.array(screenshot)\n",
    "\n",
    "        # Resize and convert the image\n",
    "        downscale_factor = 2\n",
    "        resized_image = cv2.resize(\n",
    "            screenshot_np,\n",
    "            None,\n",
    "            fx=1 / downscale_factor,\n",
    "            fy=1 / downscale_factor,\n",
    "            interpolation=cv2.INTER_AREA\n",
    "        )\n",
    "        rgb_image = cv2.cvtColor(resized_image, cv2.COLOR_BGRA2RGB)\n",
    "\n",
    "        # Preprocess image\n",
    "        img_tensor = transform(Image.fromarray(rgb_image)).unsqueeze(0) # Add batch dimension\n",
    "\n",
    "        # Make predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(img_tensor)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            prediction_idx = predicted.item()\n",
    "\n",
    "        # Map prediction index to action\n",
    "        class_names = dataset_dict['train'].classes\n",
    "        \n",
    "        action = class_names[prediction_idx]\n",
    "\n",
    "        # Simulate key press if necessary\n",
    "        acc.add(action)\n",
    "        if action != \"noop\":\n",
    "            acc.add(action)\n",
    "            press_key(action)\n",
    "            time.sleep(0.6) # manual bottleneck to prevent rapid movements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in model.named_parameters():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
