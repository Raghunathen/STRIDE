{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10d24d430>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "\n",
    "torch.manual_seed(69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    filename=\"app.log\",         # File to log to\n",
    "    level=logging.INFO,         # Logging level\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",  # Log message format\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\" # Date and time format\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downscaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Images already downscaled using trainer.\n",
    "\n",
    "def downscale_images(input_root, output_root, downscale_factor):\n",
    "    for root, dirs, files in os.walk(input_root):\n",
    "        # Create corresponding directory in the output structure\n",
    "        relative_path = os.path.relpath(root, input_root)\n",
    "        output_dir = os.path.join(output_root, relative_path)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        for file in tqdm(files):\n",
    "            if file.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'tiff', 'gif')):\n",
    "                input_path = os.path.join(root, file)\n",
    "                output_path = os.path.join(output_dir, file)\n",
    "                try:\n",
    "                    # Read and downscale the image\n",
    "                    image = cv2.imread(input_path)\n",
    "                    if image is not None:\n",
    "                        downsampled_image = cv2.Canny(cv2.resize(\n",
    "                        image,\n",
    "                        None,\n",
    "                        fx=1 / downscale_factor,\n",
    "                        fy=1 / downscale_factor,\n",
    "                        interpolation=cv2.INTER_AREA\n",
    "                    ), 200, 200)\n",
    "                        # Save the downscaled image\n",
    "                        cv2.imwrite(output_path, downsampled_image)\n",
    "                        #print(f\"Processed: {input_path} -> {output_path}\")\n",
    "                    else:\n",
    "                        #print(f\"Failed to read: {input_path}\")\n",
    "                        pass\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {input_path}: {e}\")\n",
    "            else:\n",
    "                #print(f\"Skipped non-image file: {file}\")\n",
    "                pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_root = \"Dataset\"  # Input dataset root directory\n",
    "    output_root = \"canny_images\"  # Output dataset root directory\n",
    "    fact = 10\n",
    "    downscale_images(input_root, output_root, downscale_factor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "splitfolders.ratio(\"training_images\", output=\"Dataset\",\n",
    "    seed=1337, ratio=(.8, .1, .1), group_prefix=None, move=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to dataset\n",
    "data_dir = \"canny_images\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "val_dir = os.path.join(data_dir, \"val\")\n",
    "test_dir = os.path.join(data_dir, \"test\") \n",
    "\n",
    "# PASTE YOUR TRANSFORMATIONS BELOW\n",
    "transform = {\n",
    "    \"train\": transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor() \n",
    "]) ,\n",
    "    \"val\": transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor() \n",
    "]),\n",
    "    \"test\": transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor() \n",
    "])\n",
    "}\n",
    "\n",
    "# Load datasets\n",
    "dataset_dict = {\n",
    "    \"train\": datasets.ImageFolder(train_dir, transform=transform[\"train\"]),\n",
    "    \"val\": datasets.ImageFolder(val_dir, transform=transform[\"val\"]),\n",
    "    \"test\": datasets.ImageFolder(test_dir, transform=transform[\"test\"]) \n",
    "}\n",
    "\n",
    "# Create DataLoaders\n",
    "dataloaders = {\n",
    "    \"train\": DataLoader(dataset_dict[\"train\"], batch_size=32, shuffle=True, num_workers=4),\n",
    "    \"val\": DataLoader(dataset_dict[\"val\"], batch_size=32, shuffle=False, num_workers=4),\n",
    "    \"test\": DataLoader(dataset_dict[\"test\"], batch_size=32, shuffle=False, num_workers=4)  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastCNN(nn.Module):\n",
    "    def __init__(self, num_categories):\n",
    "        super(FastCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=64,\n",
    "            kernel_size=3,\n",
    "            padding=0\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=64,\n",
    "            out_channels=128,\n",
    "            kernel_size=3,\n",
    "            padding=0\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=128,\n",
    "            out_channels=256,\n",
    "            kernel_size=3,\n",
    "            padding=0\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=256,\n",
    "            out_channels=512,\n",
    "            kernel_size=3,\n",
    "            padding=0\n",
    "        )\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(73728, 128)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc2 = nn.Linear(128, num_categories)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # x : 224 * 224 * 3\n",
    "\n",
    "        x = torch.tanh(self.conv1(x))  # 222 * 222 * 32\n",
    "        x = F.avg_pool2d(x, 2) # 111 * 111 * 32\n",
    "        \n",
    "        x = torch.tanh(self.conv2(x)) # 109 * 109 * 32\n",
    "        x = F.avg_pool2d(x, 2) # 54 * 54 * 32\n",
    "        \n",
    "        x = torch.tanh(self.conv3(x)) # 52 * 52 * 32\n",
    "        x = F.avg_pool2d(x, 2) # 26 * 26 * 32\n",
    "\n",
    "        x = torch.tanh(self.conv4(x)) # 24 * 24 * 8\n",
    "        x = F.avg_pool2d(x, 2) # 12 * 12 * 8\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Will flatten to (32, 1152)\n",
    "        \n",
    "        # Dense layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def get_model(num_categories):\n",
    "    return FastCNN(num_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "fast = FastCNN(5)\n",
    "model = get_model(5)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use it to load the trained weights or train it yourself in the below cell\n",
    "# model.load_state_dict( torch.load(\"model_weights.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 5/11 [28:47<34:33, 345.57s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Disable gradient calculations for validation\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Move inputs and labels to the appropriate device\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Forward pass\u001b[39;49;00m\n",
      "File \u001b[0;32m~/Desktop/ML Projects/SubSurferAI/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/Desktop/ML Projects/SubSurferAI/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1437\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1435\u001b[0m     \u001b[38;5;66;03m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[39;00m\n\u001b[1;32m   1436\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent_workers:\n\u001b[0;32m-> 1437\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shutdown_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1438\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m \u001b[38;5;66;03m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m \n\u001b[1;32m   1442\u001b[0m \u001b[38;5;66;03m# Check if the next sample has already been generated\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ML Projects/SubSurferAI/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1568\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._shutdown_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1563\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mark_worker_as_unavailable(worker_id, shutdown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers:\n\u001b[1;32m   1565\u001b[0m     \u001b[38;5;66;03m# We should be able to join here, but in case anything went\u001b[39;00m\n\u001b[1;32m   1566\u001b[0m     \u001b[38;5;66;03m# wrong, we set a timeout and if the workers fail to join,\u001b[39;00m\n\u001b[1;32m   1567\u001b[0m     \u001b[38;5;66;03m# they are killed in the `finally` block.\u001b[39;00m\n\u001b[0;32m-> 1568\u001b[0m     \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMP_STATUS_CHECK_INTERVAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues:\n\u001b[1;32m   1570\u001b[0m     q\u001b[38;5;241m.\u001b[39mcancel_join_thread()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/popen_fork.py:40\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultiprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wait\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentinel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/connection.py:1136\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1136\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "g = \"v5.3\"\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "noop_class_index = dataset_dict[\"train\"].class_to_idx[\"noop\"]  # Index of the \"noop\" class\n",
    "\n",
    "# Get indices for all \"noop\" samples\n",
    "noop_indices = [i for i, (_, label) in enumerate(dataset_dict[\"train\"].samples) if label == noop_class_index]\n",
    "\n",
    "def get_random_noop_subset(base_dataset, noop_indices, num_samples=1000):\n",
    "    # Randomly sample 1000 indices from the \"noop\" indices\n",
    "    selected_noop_indices = random.sample(noop_indices, num_samples)\n",
    "\n",
    "    # Combine the noop subset with the rest of the dataset\n",
    "    non_noop_indices = [i for i in range(len(base_dataset)) if i not in noop_indices]\n",
    "    final_indices = non_noop_indices + selected_noop_indices\n",
    "\n",
    "    # Return a Subset dataset\n",
    "    return Subset(base_dataset, final_indices)\n",
    "\n",
    "for epoch in trange(num_epochs+1):\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    current_train_dataset = get_random_noop_subset(dataset_dict[\"train\"], noop_indices)\n",
    "\n",
    "    # Create a DataLoader\n",
    "    train_loader = DataLoader(current_train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += torch.sum(preds == labels.data)\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(dataloaders[\"train\"].dataset)\n",
    "    train_acc = correct.double() / total\n",
    "    logging.info(f\"{epoch} : Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        # Save model weights\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        save_path = f\"Trained_Weights/{g}_{epoch}th.pth\"\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "\n",
    "        # Initialize validation metrics\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculations for validation\n",
    "            for inputs, labels in dataloaders[\"val\"]:\n",
    "                # Move inputs and labels to the appropriate device\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Update loss and accuracy\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)  # Get predicted class indices\n",
    "                correct += torch.sum(preds == labels)\n",
    "                total += labels.size(0)\n",
    "\n",
    "        # Calculate validation loss and accuracy\n",
    "        val_loss = running_loss / len(dataloaders[\"val\"].dataset)\n",
    "        val_acc = correct.double() / total\n",
    "\n",
    "        # Log validation metrics\n",
    "        logging.info(f\"Epoch {epoch}: Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in dataloaders[\"test\"]:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += torch.sum(preds == labels.data)\n",
    "        total += labels.size(0)\n",
    "\n",
    "test_loss = running_loss / len(dataloaders[\"test\"].dataset)\n",
    "test_acc = correct.double() / total\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict( torch.load(\"Trained_Weights/v5.1_5th.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 83\u001b[0m\n\u001b[1;32m     81\u001b[0m acc\u001b[38;5;241m.\u001b[39madd(action)\n\u001b[1;32m     82\u001b[0m press_key(action)\n\u001b[0;32m---> 83\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# manual bottleneck to prevent rapid movements\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mss\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from pynput.keyboard import Controller, Key\n",
    "from PIL import Image\n",
    "import time\n",
    "# Initialize keyboard controller\n",
    "keyboard = Controller()\n",
    "\n",
    "# PASTE YOUR TRANSFORMATIONS HERE\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor() \n",
    "])\n",
    "\n",
    "# Monitor region for screen capture\n",
    "monitor = {\n",
    "    \"left\": 350,  # x-coordinate for the top-left corner\n",
    "    \"top\": 180,   # y-coordinate for the top-left corner\n",
    "    \"width\": 740, # width of the capture region\n",
    "    \"height\": 690 # height of the capture region\n",
    "}\n",
    "acc=set()\n",
    "# Key mapping for predictions\n",
    "key_map = {\n",
    "    \"left\": Key.left,\n",
    "    \"right\": Key.right,\n",
    "    \"up\": Key.up,\n",
    "    \"down\": Key.down\n",
    "}\n",
    "\n",
    "# Function to simulate key presses\n",
    "def press_key(action):\n",
    "    if action in key_map:\n",
    "        keyboard.press(key_map[action])\n",
    "        keyboard.release(key_map[action])\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Capture and process the screen\n",
    "with mss.mss() as sct:\n",
    "    i = 0\n",
    "    while True:\n",
    "        # Capture screen region\n",
    "        screenshot = sct.grab(monitor)\n",
    "        i+=1\n",
    "        screenshot_np = np.array(screenshot)\n",
    "\n",
    "        # Resize and convert the image\n",
    "        downscale_factor = 2\n",
    "        resized_image = cv2.Canny(cv2.resize(\n",
    "            screenshot_np,\n",
    "            None,\n",
    "            fx=1 / downscale_factor,\n",
    "            fy=1 / downscale_factor,\n",
    "            interpolation=cv2.INTER_AREA\n",
    "        ), 200, 200)\n",
    "        rgb_image = cv2.cvtColor(resized_image, cv2.COLOR_BGRA2RGB)\n",
    "\n",
    "        # Preprocess image\n",
    "        img_tensor = transform(Image.fromarray(rgb_image)).unsqueeze(0) # Add batch dimension\n",
    "\n",
    "        # Make predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(img_tensor)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            prediction_idx = predicted.item()\n",
    "\n",
    "        # Map prediction index to action\n",
    "        class_names = dataset_dict['train'].classes\n",
    "        \n",
    "        action = class_names[prediction_idx]\n",
    "\n",
    "        # Simulate key press if necessary\n",
    "        acc.add(action)\n",
    "        if action != \"noop\":\n",
    "            acc.add(action)\n",
    "            press_key(action)\n",
    "            time.sleep(1) # manual bottleneck to prevent rapid movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in model.named_parameters():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradCAM (experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "# Load the image\n",
    "image_path = \"dummy.png\"  # Replace with your image path\n",
    "image = Image.open(image_path)  # Convert to RGB if it's not already\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Resize image to 64x64\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()         # Convert image to PyTorch tensor\n",
    "])\n",
    "\n",
    "# Apply transformations\n",
    "image_tensor = transform(image)\n",
    "\n",
    "# Add batch dimension if needed\n",
    "image_tensor = image_tensor.unsqueeze(0)  # Shape: (1, C, H, W)\n",
    "\n",
    "print(f\"Tensor shape: {image_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import ToPILImage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "\n",
    "        # Hook the gradients of the target layer\n",
    "        self.target_layer.register_backward_hook(self.save_gradients)\n",
    "        self.target_layer.register_forward_hook(self.save_activations)\n",
    "\n",
    "    def save_gradients(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0]\n",
    "\n",
    "    def save_activations(self, module, input, output):\n",
    "        self.activations = output\n",
    "\n",
    "    def generate_heatmap(self, input_tensor, target_class):\n",
    "        # Forward pass\n",
    "        output = self.model(input_tensor)\n",
    "\n",
    "        # Backward pass to get gradients of the target class\n",
    "        self.model.zero_grad()\n",
    "        target = output[:, target_class]\n",
    "        target.backward()\n",
    "\n",
    "        # Compute Grad-CAM\n",
    "        weights = self.gradients.mean(dim=(2, 3), keepdim=True)  # Average over spatial dimensions\n",
    "        cam = (weights * self.activations).sum(dim=1).squeeze()  # Weighted sum over channels\n",
    "\n",
    "        # ReLU to retain positive influences only\n",
    "        cam = F.relu(cam)\n",
    "        cam = cam - cam.min()  # Normalize\n",
    "        cam = cam / cam.max()\n",
    "\n",
    "        return cam\n",
    "\n",
    "def plot_heatmap(cam, original_image):\n",
    "    cam = cam.detach().cpu().numpy()\n",
    "\n",
    "    plt.imshow(original_image.permute(1, 2, 0))  # Assuming original_image is a 3D tensor\n",
    "    plt.imshow(cam, cmap=\"jet\", alpha=0.5)  # Overlay heatmap\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Example Usage\n",
    "grad_cam = GradCAM(model, model.conv1)\n",
    "\n",
    "input_tensor = image_tensor\n",
    "target_class = 3  # Specify the target class for which you want to visualize Grad-CAM\n",
    "\n",
    "cam = grad_cam.generate_heatmap(input_tensor, target_class)\n",
    "plot_heatmap(cam, input_tensor.squeeze(0))  # Provide a corresponding image for overlay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
